#include <algorithm>
#include <functional>
#include <iterator>
#include <map>
#include <memory>
#include <typeinfo>
#include <utility>
#include <vector>
namespace test {
/**
* @brief A kind of message manager supporting auto parallelism.
*
* After registering the vertex array and message strategy as a sync buffer,
* message generation and ingestion can be applied by message manager
* automatically.
*/
template <typename TEST_T>
class AutoParallelMessageManager : public DefaultMessageManager {
using Base = DefaultMessageManager;
using Ver_T = typename TEST_T::Ver_T;
struct ap_event {
ap_event(const TEST_T& f, ISyncBuffer* b, MessageStrategy m, int e)
: fragment(f), buffer(b), message_strategy(m), event_id(e) {}
const TEST_T& fragment;
ISyncBuffer* buffer;
MessageStrategy message_strategy;
int event_id;
};
public:
AutoParallelMessageManager() {}
~AutoParallelMessageManager() override {}
using Base::Init;
using Base::Start;
/**
* @brief Inherit
*/
void StartARound() override {
Base::StartARound();
aggregateAutoMessages();
}
/**
* @brief Inherit
*/
void FinishARound() override {
generateAutoMessages();
Base::FinishARound();
}
using Base::ToTerminate;
using Base::Finalize;
using Base::GetMsgSize;
using Base::ContinueProcessing;
/**
* @brief Register a buffer to be sync automatically between rounds.
*
* @param frag
* @param buffer
* @param strategy
*/
inline void RegisterSyncBuffer(const TEST_T& frag, ISyncBuffer* buffer,
MessageStrategy strategy) {
int event_id = auto_parallel_events_.size();
auto_parallel_events_.emplace_back(frag, buffer, strategy, event_id);
}
private:
void aggregateAutoMessages() {
std::map<int, ap_event*> event_map;
for (auto& event : auto_parallel_events_) {
event_map.emplace(event.event_id, &event);
}
int event_id;
while (Base::GetMessage<int>(event_id)) {
ap_event* event = event_map.at(event_id);
auto& i_ec_frag = event->fragment;
if (event->message_strategy == MessageStrategy::kSyncOnOuterVertex ||
event->message_strategy == MessageStrategy::kAlongEdgeToOuterVertex ||
event->message_strategy ==
MessageStrategy::kAlongOutgoingEdgeToOuterVertex ||
event->message_strategy ==
MessageStrategy::kAlongIncomingEdgeToOuterVertex) {
if (event->buffer->GetTypeId() == typeid(double)) {
syncOnVertexRecv<double>(i_ec_frag, event->buffer);
} else if (event->buffer->GetTypeId() == typeid(uint32_t)) {
syncOnVertexRecv<uint32_t>(i_ec_frag, event->buffer);
} else if (event->buffer->GetTypeId() == typeid(int32_t)) {
syncOnVertexRecv<int32_t>(i_ec_frag, event->buffer);
}
} else if (event->buffer->GetTypeId() == typeid(int64_t)) {
syncOnVertexRecv<int64_t>(i_ec_frag, event->buffer);
} else if (event->buffer->GetTypeId() == typeid(uint64_t)) {
syncOnVertexRecv<uint64_t>(i_ec_frag, event->buffer);
} else if (event->buffer->GetTypeId() ==
typeid(std::vector<uint32_t>)) {
syncOnVertexRecv<std::vector<uint32_t>>(i_ec_frag, event->buffer);
} else if (event->buffer->GetTypeId() ==
typeid(std::vector<uint64_t>)) {
syncOnVertexRecv<std::vector<uint64_t>>(i_ec_frag, event->buffer);
} else {
LOG(FATAL) << "Unexpected data type "
<< event->buffer->GetTypeId()
name();
}
} else {
LOG(FATAL) << "Unexpected message stratety "
<< underlying_value(event->message_strategy);
}
}
}
void generateAutoMessages() {
for (auto& event_ref : auto_parallel_events_) {
ap_event* event = &event_ref;
auto& i_ec_frag = event->fragment;
auto inner_size = i_ec_frag.InnerVertices().size();
if (event->buffer->updated(0, inner_size)) {
ContinueProcessing();
break;
}
}
for (auto& event_ref : auto_parallel_events_) {
ap_event* event = &event_ref;
auto& i_ec_frag = event->fragment;
if (event->message_strategy == MessageStrategy::kSyncOnOuterVertex) {
if (event->buffer->GetTypeId() == typeid(double)) {
syncOnOuterVertexSend<double>(i_ec_frag, event->buffer,
event->event_id);
} else if (event->buffer->GetTypeId() == typeid(uint32_t)) {
syncOnOuterVertexSend<uint32_t>(i_ec_frag, event->buffer,
event->event_id);
} else if (event->buffer->GetTypeId() == typeid(int32_t)) {
syncOnOuterVertexSend<int32_t>(i_ec_frag, event->buffer,
event->event_id);
} else if (event->buffer->GetTypeId() == typeid(int64_t)) {
syncOnOuterVertexSend<int64_t>(i_ec_frag, event->buffer,
event->event_id);
} else if (event->buffer->GetTypeId() == typeid(uint64_t)) {
syncOnOuterVertexSend<uint64_t>(i_ec_frag, event->buffer,
event->event_id);
} else {
LOG(FATAL) << "Unexpected data type for auto parallelization: "
<< event->buffer->GetTypeId()
name();
}
} else if (event->message_strategy ==
MessageStrategy::kAlongEdgeToOuterVertex ||
event->message_strategy ==
MessageStrategy::kAlongIncomingEdgeToOuterVertex ||
event->message_strategy ==
MessageStrategy::kAlongOutgoingEdgeToOuterVertex) {
if (event->buffer->GetTypeId() == typeid(double)) {
syncOnInnerVertexSend<double>(i_ec_frag, event->buffer,
event->event_id,
event->message_strategy);
} else if (event->buffer->GetTypeId() == typeid(uint32_t)) {
syncOnInnerVertexSend<uint32_t>(i_ec_frag, event->buffer,
event->event_id,
event->message_strategy);
} else if (event->buffer->GetTypeId() == typeid(int32_t)) {
syncOnInnerVertexSend<int32_t>(i_ec_frag, event->buffer,
event->event_id,
event->message_strategy);
} else if (event->buffer->GetTypeId() == typeid(int64_t)) {
syncOnInnerVertexSend<int64_t>(i_ec_frag, event->buffer,
event->event_id,
event->message_strategy);
} else if (event->buffer->GetTypeId() == typeid(uint64_t)) {
syncOnInnerVertexSend<uint64_t>(i_ec_frag, event->buffer,
event->event_id,
event->message_strategy);
} else if (event->buffer->GetTypeId() ==
typeid(std::vector<uint32_t>)) {
syncOnInnerVertexSend<std::vector<uint32_t>>(i_ec_frag, event->buffer,
event->event_id,
event->message_strategy);
} else if (event->buffer->GetTypeId() ==
typeid(std::vector<uint64_t>)) {
syncOnInnerVertexSend<std::vector<uint64_t>>(i_ec_frag, event->buffer,
event->event_id,
event->message_strategy);
} else {
LOG(FATAL) << "Unexpected data type for auto parallelization: "
<< event->buffer->GetTypeId()
name();
}
} else {
LOG(FATAL) << "Unexpected message stratety "
<< underlying_value(event->message_strategy);
}
}
}
template <typename T>
inline void syncOnInnerVertexSend(const TEST_T& frag, ISyncBuffer* buffer,
int event_id,
MessageStrategy message_strategy) {
auto* bptr =
dynamic_cast<SyncBuffer<typename TEST_T::vertices_t, T>*>(buffer);
auto inner_vertices = frag.InnerVertices();
std::vector<size_t> message_num(Base::fnum(), 0);
if (message_strategy == MessageStrategy::kAlongEdgeToOuterVertex) {
for (auto v : inner_vertices) {
if (bptr->IsUpdated(v)) {
auto dsts = frag.IOEDests(v);
const fid_t* ptr = dsts.begin;
while (ptr != dsts.end) {
++message_num[*(ptr++)];
}
}
}
} else if (message_strategy ==
MessageStrategy::kAlongIncomingEdgeToOuterVertex) {
for (auto v : inner_vertices) {
if (bptr->IsUpdated(v)) {
auto dsts = frag.IEDests(v);
const fid_t* ptr = dsts.begin;
while (ptr != dsts
end) {
++message_num[*(ptr++)];
}
}
}
} else if (message_strategy ==
MessageStrategy::kAlongOutgoingEdgeToOuterVertex) {
for (auto v : inner_vertices) {
if (bptr->IsUpdated(v)) {
auto dsts = frag.OEDests(v);
const fid_t* ptr = dsts.begin;
while (ptr != dsts.end) {
++message_num[*(ptr++)];
}
}
}
}
for (fid_t i = 0; i < Base::fnum(); i++) {
if (message_num[i] > 0) {
Base::SendToFragment<int>(i, event_id);
Base::SendToFragment<size_t>(i, message_num[i]);
}
}
if (message_strategy == MessageStrategy::kAlongEdgeToOuterVertex) {
for (auto v : inner_vertices) {
if (bptr->IsUpdated(v)) {
Base::SendMsgThroughEdges(frag, v, bptr->GetValue(v));
bptr->Reset(v);
}
}
} else if (message_strategy ==
MessageStrategy::kAlongIncomingEdgeToOuterVertex) {
for (auto v : inner_vertices) {
if (bptr->IsUpdated(v)) {
Base::SendMsgThroughIEdges(frag, v, bptr->GetValue(v));
bptr->Reset(v);
}
}
} else if (message_strategy ==
MessageStrategy::kAlongOutgoingEdgeToOuterVertex) {
for (auto v : inner_vertices) {
if (bptr->IsUpdated(v)) {
Base::SendToNeighbors(frag, v, bptr->GetValue(v));
bptr->Reset(v);
}
}
}
}
template <typename T>
inline void syncOnOuterVertexSend(const TEST_T& frag, ISyncBuffer* buffer,
int event_id) {
auto* bptr =
dynamic_cast<SyncBuffer<typename TEST_T::vertices_t, T>*>(buffer);
auto inner_vertices = frag.InnerVertices();
auto outer_vertices = frag.OuterVertices();
std::vector<size_t> message_num(Base::fnum(), 0);
for (auto v : inner_vertices) {
bptr->Reset(v);
}
for (auto v : outer_vertices) {
if (bptr->IsUpdated(v)) {
fid_t fid = frag.GetFragId(v);
++message_num[fid];
}
}
for (fid_t i = 0; i < Base::fnum(); i++) {
if (message_num[i] > 0) {
Base::SendToFragment<int>(i, event_id);
Base::SendToFragment<size_t>(i, message_num[i]);
}
}
for (auto v : outer_vertices) {
if (bptr->IsUpdated(v)) {
Base::SyncStateOnOuterVertex(frag, v, bptr->GetValue(v));
bptr->Reset(v);
}
}
}
template <typename T>
inline void syncOnVertexRecv(const TEST_T& frag, ISyncBuffer* buffer) {
auto* bptr =
dynamic_cast<SyncBuffer<typename TEST_T::vertices_t, T>*>(buffer);
size_t message_num = 0;
T rhs;
Vertex<Ver_T> v(0);
Base::GetMessage<size_t>(message_num);
while (message_num--) {
GetMessage(frag, v, rhs);
bptr->Aggregate(v, std::move(rhs));
}
}
std::vector<ap_event> auto_parallel_events_;
}; // namespace platform
} // namespace platform
#endif // platform_PARALLEL_AUTO_PARALLEL_MESSAGE_MANAGER_H_

ifndef platform_PARALLEL_BATCH_SHUFFLE_MESSAGE_MANAGER_H_
#define platform_PARALLEL_BATCH_SHUFFLE_MESSAGE_MANAGER_H_

#include <memory>
#include <thread>
#include <vector>

#include "platform/communication/sync_comm.h"
#include "platform/parallel/message_manager_base.h"
#include "platform/utils/vertex_array.h"
#include "platform/worker/comm_spec.h"

namespace platform {

/**
* @brief A kind of collective message manager.
*
* This message manager is designed for the scenario that all mirror vertices'
* state need to be override with their masters' state, e.g. PageRank.
*
* After a round, message manager will encode the inner vertices' state of a
* vertex array for each other fragment.
*
* When receive a batch of messages, message manager will update the state of
* outer vertices in a designated vertex array.
*/

namespace batch_shuffle_message_manager_impl {

template <typename TEST_T>
struct IsRange {
using sub_vertices_t = typename TEST_T::sub_vertices_t;
using Ver_T = typename TEST_T::Ver_T;
static constexpr bool value =
std::is_same<sub_vertices_t, VertexRange<Ver_T>>::value;
};

template <typename TEST_T, typename MESSAGE_T>
struct ShuffleInplace {
static constexpr bool value =
IsRange<TEST_T>::value && std::is_pod<MESSAGE_T>::value;
};

template <typename TEST_T, typename MESSAGE_T>
struct PodShuffle {
static constexpr bool value =
!IsRange<TEST_T>::value && std::is_pod<MESSAGE_T>::value;
};

template <typename MESSAGE_T>
struct ArchiveShuffle {
static constexpr bool value = !std::is_pod<MESSAGE_T>::value;
};

class PostProcessBase {
public:
PostProcessBase() {}
virtual ~PostProcessBase() {}

virtual void exec(fid_t fid) = 0;
};

template <typename GRAPH_T, typename DATA_T>
class PostProcess : public PostProcessBase {
using array_t = typename GRAPH_T::template vertex_array_t<DATA_T>;

public:
PostProcess(const GRAPH_T& frag, array_t& data,
std::vector<std::vector<char, Allocator<char>>>& buffers)
: frag_(frag), data_(data), buffers_(buffers) {}

void exec(fid_t fid) {
if (fid == frag_.fid()) {
return;
}

auto& vec = buffers_[fid];
OutArchive arc;
arc.SetSlice(vec.data(), vec.size());
auto& vertices = frag_.OuterVertices(fid);
for (auto v : vertices) {
arc >> data_[v];
}
buffers_[fid].clear();
}

private:
const GRAPH_T& frag_;
array_t& data_;
std::vector<std::vector<char, Allocator<char>>>& buffers_;
};

} // namespace batch_shuffle_message_manager_impl

class BatchShuffleMessageManager : public MessageManagerBase {
template <typename TEST_T, typename MESSAGE_T>
using shuffle_inplace_t =
batch_shuffle_message_manager_impl::ShuffleInplace<TEST_T, MESSAGE_T>;

template <typename TEST_T, typename MESSAGE_T>
using pod_shuffle_t =
batch_shuffle_message_manager_impl::PodShuffle<TEST_T, MESSAGE_T>;

template <typename TEST_T>
using archive_shuffle_t =
batch_shuffle_message_manager_impl::ArchiveShuffle<TEST_T>;

public:
BatchShuffleMessageManager() : comm_(NULL_COMM) {}
~BatchShuffleMessageManager() {
if (ValidComm(comm_)) {
MPI_Comm_free(&comm_);
}
}

/**
* @brief Inherit
*/
void Init(MPI_Comm comm) override {
MPI_Comm_dup(comm, &comm_);

comm_spec_.Init(comm_);
fid_ = comm_spec_.fid();
fnum_ = comm_spec_.fnum();

remaining_reqs_.resize(fnum_);

force_terminate_ = false;
terminate_info_.Init(fnum_);

shuffle_out_buffers_.resize(fnum_);
shuffle_in_buffers_.resize(fnum_);
recv_thread_ =
std::thread(&BatchShuffleMessageManager::recvThreadRoutine, this);
}

void ReserveBuffer(fid_t fid, size_t send_size, size_t recv_size) {
shuffle_out_buffers_[fid].reserve(send_size);
shuffle_in_buffers_[fid].reserve(recv_size);
}

void SetupBuffer(fid_t fid, std::vector<char, Allocator<char>>&& send_buffer,
std::vector<char, Allocator<char>>&& recv_buffer) {
shuffle_out_buffers_[fid] = std::move(send_buffer);
shuffle_in_buffers_[fid] = std::move(recv_buffer);
}

/**
* @brief Inherit
*/
void Start() override {}

/**
* @brief Inherit
*/
void StartARound() override {
msg_size_ = 0;
to_terminate_ = true;
}

/**
* @brief Inherit
*/
void FinishARound() override {}

/**
* @brief Inherit
*/
bool ToTerminate() override {
int flag = force_terminate_ ? 1 : 0;
int ret;
MPI_Allreduce(&flag, &ret, 1, MPI_INT, MPI_SUM, comm_);
if (ret > 0) {
terminate_info_.success = false;
sync_comm::AllGather(terminate_info_.info, comm_);
return true;
}
return to_terminate_;
}

/**
* @brief Inherit
*/
size_t GetMsgSize() const override { return msg_size_; }

/**
* @brief Inherit
*/
void Finalize() override {
if (!send_reqs_.empty()) {
MPI_Waitall(send_reqs_.size(), &send_reqs_[0], MPI_STATUSES_IGNORE);
send_reqs_.clear();
}

if (!recv_reqs_.empty()) {
MPI_Waitall(recv_reqs_.size(), &recv_reqs_[0], MPI_STATUSES_IGNORE);
recv_reqs_.clear();
}

{
size_t v = 1;
MPI_Send(&v, sizeof(size_t), MPI_CHAR, comm_spec_.FragToWorker(fid_), 1,
comm_);
recv_thread_.join();
}

MPI_Comm_free(&comm_);
comm_ = NULL_COMM;
}

/**
* @brief Synchronize the inner vertices' data of a vertex array to their
* mirrors.
*
* @tparam GRAPH_T
* @tparam DATA_T
* @param frag
* @param data
*/
template <typename GRAPH_T, typename DATA_T>
void SyncInnerVertices(
const GRAPH_T& frag,
typename GRAPH_T::template vertex_array_t<DATA_T>& data,
int thread_num = std::thread::hardware_concurrency()) {
to_terminate_ = false;
if (!send_reqs_.empty()) {
MPI_Waitall(send_reqs_.size(), &send_reqs_[0], MPI_STATUSES_IGNORE);
send_reqs_.clear();
}

if (!recv_reqs_.empty()) {
MPI_Waitall(recv_reqs_.size(), &recv_reqs_[0], MPI_STATUSES_IGNORE);
recv_reqs_.clear();
recv_from_.clear();
}

startRecv<GRAPH_T, DATA_T>(frag, data, thread_num);

remaining_frags_ = fnum_ - 1;

startSend<GRAPH_T, DATA_T>(frag, data, thread_num);
}

/**
* @brief This function will block until all outer vertices are updated, that
* is, messages from all other fragments are received.
*/
void UpdateOuterVertices() {
MPI_Waitall(recv_reqs_.size(), &recv_reqs_[0], MPI_STATUSES_IGNORE);
}

/**
* @brief This function will block until a set of messages from one fragment
* are received.
*
* @return Source fragment id.
*/
fid_t UpdatePartialOuterVertices() {
int index;
fid_t ret;
while (true) {
MPI_Waitany(recv_reqs_.size(), &recv_reqs_[0], &index, MPI_STATUS_IGNORE);
ret = recv_from_[index];
--remaining_reqs_[ret];
if (remaining_reqs_[ret] == 0) {
--remaining_frags_;
if (remaining_frags_ == 0) {
recv_reqs_.clear();
recv_from_.clear();
}
break;
}
}
if (post_process_handle_ != nullptr) {
post_process_handle_->exec(ret);
}
return ret;
}

/**
* @brief Inherit
*/
void ContinueProcessing() override {}

/**
* @brief Inherit
*/
void ForceTerminate(const std::string& terminate_info) override {
force_terminate_ = true;
terminate_info_.info[comm_spec_.fid()] = terminate_info;
}

/**
* @brief Inherit
*/
const TerminateInfo& GetTerminateInfo() const override {
return terminate_info_;
}

private:
void recvThreadRoutine() {
std::vector<MPI_Request> recv_thread_reqs(fnum_);
std::vector<size_t> numbers(fnum_);
for (fid_t src_fid = 0; src_fid < fnum_; ++src_fid) {
MPI_Irecv(&numbers[src_fid], sizeof(size_t), MPI_CHAR,
comm_spec_.FragToWorker(src_fid), 1, comm_,
&recv_thread_reqs[src_fid]);
}
int index;
MPI_Waitany(fnum_, &recv_thread_reqs[0], &index, MPI_STATUS_IGNORE);
CHECK(index == static_cast<int>(fid_));
for (fid_t src_fid = 0; src_fid < fnum_; ++src_fid) {
if (src_fid != fid_) {
MPI_Cancel(&recv_thread_reqs[src_fid]);
}
}
}

template <typename GRAPH_T, typename DATA_T>
typename std::enable_if<archive_shuffle_t<DATA_T>::value>::type startRecv(
const GRAPH_T& frag,
typename GRAPH_T::template vertex_array_t<DATA_T>& data, int thread_num) {
std::vector<std::thread> threads(thread_num);
std::atomic<fid_t> cur_fid(0);
std::vector<size_t> out_archive_sizes(fnum_), in_archive_sizes(fnum_);
for (int i = 0; i < thread_num; ++i) {
threads[i] = std::thread([&]() {
while (true) {
fid_t got = cur_fid.fetch_add(1);
if (got >= fnum_) {
break;
}
auto& arc = shuffle_out_archives_[got];
arc.Clear();
auto& v_set = frag.MirrorVertices(got);
for (auto v : v_set) {
arc << data[v];
}
out_archive_sizes[comm_spec_.FragToWorker(got)] = arc.GetSize();
}
});
}
for (auto& thrd : threads) {
thrd.join();
}

sync_comm::AllToAll(out_archive_sizes, in_archive_sizes, comm_spec_.comm());

for (fid_t i = 1; i < fnum_; ++i) {
fid_t src_fid = (fid_ + fnum_ - i) % fnum_;
auto& buffer = shuffle_in_buffers_[src_fid];
buffer.resize(in_archive_sizes[src_fid]);
int old_req_num = recv_reqs_.size();
sync_comm::irecv_buffer<char>(buffer.data(), buffer.size(),
comm_spec_.FragToWorker(src_fid), 0, comm_,
recv_reqs_);
int new_req_num = recv_reqs_.size();
recv_from_.resize(new_req_num, src_fid);
remaining_reqs_[src_fid] = new_req_num - old_req_num;
}

post_process_handle_ = std::make_shared<
batch_shuffle_message_manager_impl::PostProcess<GRAPH_T, DATA_T>>(
frag, data, shuffle_in_buffers_);
}
template <typename GRAPH_T, typename DATA_T>
typename std::enable_if<shuffle_inplace_t<GRAPH_T, DATA_T>::value>::type
startRecv(const GRAPH_T& frag,
typename GRAPH_T::template vertex_array_t<DATA_T>& data,
int thread_num) {
std::vector<int> req_offsets(fnum_);
for (fid_t i = 1; i < fnum_; ++i) {
fid_t src_fid = (fid_ + fnum_ - i) % fnum_;
auto range = frag.OuterVertices(src_fid);
int old_req_num = recv_reqs_.size();
int new_req_num =
sync_comm::chunk_num<char>(range.size() * sizeof(DATA_T)) +
old_req_num;
recv_reqs_.resize(new_req_num);
req_offsets[src_fid] = old_req_num;
recv_from_.resize(new_req_num, src_fid);
remaining_reqs_[src_fid] = new_req_num - old_req_num;
}
int fnum = static_cast<int>(fnum_);
thread_num = (fnum - 1) < thread_num ? (fnum - 1) : thread_num;
std::vector<std::thread> threads(thread_num);
std::atomic<fid_t> cur(1);
for (int i = 0; i < thread_num; ++i) {
threads[i] = std::thread([&]() {
while (true) {
fid_t got = cur.fetch_add(1);
if (got >= fnum_) {
break;
}
fid_t src_fid = (fid_ + fnum_ - got) % fnum_;
auto range = frag.OuterVertices(src_fid);
sync_comm::irecv_buffer<char>(
reinterpret_cast<char*>(&data[*range.begin()]),
range.size() * sizeof(DATA_T), comm_spec_.FragToWorker(src_fid),
0, comm_, &recv_reqs_[req_offsets[src_fid]]);
}
});
}
for (auto& thrd : threads) {
thrd.join();
}
}

template <typename GRAPH_T, typename DATA_T>
typename std::enable_if<pod_shuffle_t<GRAPH_T, DATA_T>::value>::type
startRecv(const GRAPH_T& frag,
typename GRAPH_T::template vertex_array_t<DATA_T>& data,
int thread_num) {
for (fid_t i = 1; i < fnum_; ++i) {
fid_t src_fid = (fid_ + fnum_ - i) % fnum_;
auto& buffer = shuffle_in_buffers_[src_fid];
buffer.resize(frag.OuterVertices(src_fid).size() * sizeof(DATA_T));
int old_req_num = recv_reqs_.size();
sync_comm::irecv_buffer<char>(buffer.data(), buffer.size(),
comm_spec_.FragToWorker(src_fid), 0, comm_,
recv_reqs_);
int new_req_num = recv_reqs_.size();
recv_from_.resize(new_req_num, src_fid);
remaining_reqs_[src_fid] = new_req_num - old_req_num;
}
post_process_handle_ = std::make_shared<
batch_shuffle_message_manager_impl::PostProcess<GRAPH_T, DATA_T>>(
frag, data, shuffle_in_buffers_);
}

template <typename GRAPH_T, typename DATA_T>
typename std::enable_if<!archive_shuffle_t<DATA_T>::value>::type startSend(
const GRAPH_T& frag,
const typename GRAPH_T::template vertex_array_t<DATA_T>& data,
int thread_num) {
CHECK_EQ(sending_queue_.Size(), 0);
int fnum = static_cast<int>(fnum_);
sending_queue_.SetProducerNum(thread_num < fnum ? thread_num : 1);
std::thread send_thread = std::thread([this]() {
fid_t got;
while (sending_queue_.Get(got)) {
auto& vec = shuffle_out_buffers_[got];
sync_comm::isend_buffer<char>(vec.data(), vec.size(),
comm_spec_.FragToWorker(got), 0, comm_,
send_reqs_);
}
});
if (thread_num < fnum) {
std::atomic<fid_t> cur(1);
std::vector<std::thread> work_threads(thread_num);
for (int i = 0; i < thread_num; ++i) {
work_threads[i] = std::thread([&]() {
while (true) {
fid_t got = cur.fetch_add(1);
if (got >= fnum_) {
break;
}
fid_t dst_fid = (got + fid_) % fnum_;
auto& id_vec = frag.MirrorVertices(dst_fid);
auto& vec = shuffle_out_buffers_[dst_fid];
vec.clear();
vec.resize(id_vec.size() * sizeof(DATA_T));
DATA_T* buf = reinterpret_cast<DATA_T*>(vec.data());
size_t num = id_vec.size();
for (size_t k = 0; k < num; ++k) {
buf[k] = data[id_vec[k]];
}
sending_queue_.Put(dst_fid);
}
sending_queue_.DecProducerNum();
});
}
for (auto& thrd : work_threads) {
thrd.join();
}
} else {
for (fid_t i = 1; i < fnum_; ++i) {
fid_t dst_fid = (i + fid_) % fnum_;
auto& id_vec = frag.MirrorVertices(dst_fid);
auto& vec = shuffle_out_buffers_[dst_fid];
vec.clear();
vec.resize(id_vec.size() * sizeof(DATA_T));
DATA_T* buf = reinterpret_cast<DATA_T*>(vec.data());
size_t num = id_vec.size();
#pragma omp parallel for num_threads(thread_num)
for (size_t k = 0; k < num; ++k) {
buf[k] = data[id_vec[k]];
}

sending_queue_.Put(dst_fid);
msg_size_ += vec.size();
}
sending_queue_.DecProducerNum();
}
send_thread.join();
}

template <typename GRAPH_T, typename DATA_T>
typename std::enable_if<archive_shuffle_t<DATA_T>::value>::type startSend(
const GRAPH_T& frag,
const typename GRAPH_T::template vertex_array_t<DATA_T>& data,
int thread_num) {
for (fid_t i = 1; i < fnum_; ++i) {
fid_t dst_fid = (i + fid_) % fnum_;
auto& arc = shuffle_out_archives_[dst_fid];
sync_comm::isend_buffer<char>(arc.GetBuffer(), arc.GetSize(),
comm_spec_.FragToWorker(dst_fid), 0, comm_,
send_reqs_);
msg_size_ += arc.GetSize();
}
}

template <typename GRAPH_T, typename DATA_T>
typename std::enable_if<!shuffle_inplace_t<GRAPH_T, DATA_T>::value>::type
postProcess(const GRAPH_T& frag, fid_t i,
const typename GRAPH_T::template vertex_array_t<DATA_T>& data,
int thread_num) {
if (i == fid_) {
return;
}
auto& vec = shuffle_in_buffers_[i];
OutArchive arc;
arc.SetSlice(vec.data(), vec.size());
auto& vertices = frag.OuterVertices(i);
for (auto v : vertices) {
arc >> data[v];
}
}

template <typename GRAPH_T, typename DATA_T>
typename std::enable_if<shuffle_inplace_t<GRAPH_T, DATA_T>::value>::type
postProcess(const GRAPH_T& frag, fid_t i,
const typename GRAPH_T::template vertex_array_t<DATA_T>& data,
int thread_num) {}

fid_t fid_;
fid_t fnum_;
CommSpec comm_spec_;

MPI_Comm comm_;

std::vector<std::vector<char, Allocator<char>>> shuffle_out_buffers_;
std::vector<InArchive> shuffle_out_archives_;
std::vector<std::vector<char, Allocator<char>>> shuffle_in_buffers_;
std::shared_ptr<batch_shuffle_message_manager_impl::PostProcessBase>
post_process_handle_;

std::vector<MPI_Request> recv_reqs_;
std::vector<fid_t> recv_from_;
std::vector<int> remaining_reqs_;
fid_t remaining_frags_;

std::vector<MPI_Request> send_reqs_;

size_t msg_size_;
std::thread recv_thread_;

bool to_terminate_;

bool force_terminate_;
TerminateInfo terminate_info_;

BlockingQueue<fid_t> sending_queue_;
};

} // namespace platform

#endif // platform_PARALLEL_BATCH_SHUFFLE_MESSAGE_MANAGER_H_

#ifndef platform_PARALLEL_DEFAULT_MESSAGE_MANAGER_H_
#define platform_PARALLEL_DEFAULT_MESSAGE_MANAGER_H_

#include <memory>
#include <utility>
#include <vector>

#include "platform/communication/sync_comm.h"
#include "platform/parallel/message_manager_base.h"
#include "platform/serialization/in_archive.h"
#include "platform/serialization/out_archive.h"
#include "platform/worker/comm_spec.h"

namespace platform {

/**
* @brief Default message manager.
*
* The send and recv methods are not thread-safe.
*/
class DefaultMessageManager : public MessageManagerBase {
public:
DefaultMessageManager() : comm_(NULL_COMM) {}
~DefaultMessageManager() override {
if (ValidComm(comm_)) {
MPI_Comm_free(&comm_);
}
}

/**
* @brief Inherit
*/
void Init(MPI_Comm comm) override {
MPI_Comm_dup(comm, &comm_);

comm_spec_.Init(comm_);
fid_ = comm_spec_.fid();
fnum_ = comm_spec_.fnum();

force_terminate_ = false;
terminate_info_.Init(fnum_);

lengths_out_.resize(fnum_);
lengths_in_.resize(fnum_ * fnum_);

to_send_.resize(fnum_);
to_recv_.resize(fnum_);
}

/**
* @brief Inherit
*/
void Start() override {}

/**
* @brief Inherit
*/
void StartARound() override {
sent_size_ = 0;
if (!reqs_.empty()) {
MPI_Waitall(reqs_.size(), &reqs_[0], MPI_STATUSES_IGNORE);
reqs_.clear();
}
for (auto& arc : to_send_) {
arc.Clear();
}
force_continue_ = false;
cur_ = 0;
}

/**
* @brief Inherit
*/
void FinishARound() override {
to_terminate_ = syncLengths();
if (to_terminate_) {
return;
}

for (fid_t i = 1; i < fnum_; ++i) {
fid_t src_fid = (fid_ + i) % fnum_;
size_t length = lengths_in_[src_fid * fnum_ + fid_];
if (length == 0) {
continue;
}
auto& arc = to_recv_[src_fid];
arc.Clear();
arc.Allocate(length);
sync_comm::irecv_buffer<char>(arc.GetBuffer(), length,
comm_spec_.FragToWorker(src_fid), 0, comm_,
reqs_);
}
for (fid_t i = 1; i < fnum_; ++i) {
fid_t dst_fid = (fid_ + fnum_ - i) % fnum_;
auto& arc = to_send_[dst_fid];
if (arc.Empty()) {
continue;
}
sync_comm::isend_buffer<char>(arc.GetBuffer(), arc.GetSize(),
comm_spec_.FragToWorker(dst_fid), 0, comm_,
reqs_);
}
to_recv_[fid_].Clear();
if (!to_send_[fid_].Empty()) {
to_recv_[fid_] = std::move(to_send_[fid_]);
}
}

/**
* @brief Inherit
*/
bool ToTerminate() override { return to_terminate_; }

/**
* @brief Inherit
*/
void Finalize() override {
if (!reqs_.empty()) {
MPI_Waitall(reqs_.size(), &reqs_[0], MPI_STATUSES_IGNORE);
reqs_.clear();
}

MPI_Comm_free(&comm_);
comm_ = NULL_COMM;
}

/**
* @brief Inherit
*/
size_t GetMsgSize() const override { return sent_size_; }

/**
* @brief Inherit
*/
void ContinueProcessing() override { force_continue_ = true; }

/**
* @brief Inherit
*/
void ForceTerminate(const std::string& terminate_info) override {
force_terminate_ = true;
terminate_info_.info[comm_spec_.fid()] = terminate_info;
}

/**
* @brief Inherit
*/
const TerminateInfo& GetTerminateInfo() const override {
return terminate_info_;
}

/**
* @brief Send message to a fragment.
*
* @tparam MESSAGE_T Message type.
* @param dst_fid Destination fragment id.
* @param msg
*/
template <typename MESSAGE_T>
inline void SendToFragment(fid_t dst_fid, const MESSAGE_T& msg) {
to_send_[dst_fid] << msg;
}

/**
* @brief Communication by synchronizing the status on outer vertices, for
* edge-cut fragments.
*
* Assume a fragment F_1, a crossing edge a->b' in F_1 and a is an inner
* vertex in F_1. This function invoked on F_1 send status on b' to b on F_2,
* where b is an inner vertex.
*
* @tparam GRAPH_T
* @tparam MESSAGE_T
* @param frag
* @param v: a
* @param msg
*/
template <typename GRAPH_T, typename MESSAGE_T>
inline void SyncStateOnOuterVertex(const GRAPH_T& frag,
const typename GRAPH_T::vertex_t& v,
const MESSAGE_T& msg) {
fid_t fid = frag.GetFragId(v);
to_send_[fid] << frag.GetOuterVertexGid(v) << msg;
}

/**
* @brief Communication via a crossing edge a<-c. It sends message
* from a to c.
*
* @tparam GRAPH_T
* @tparam MESSAGE_T
* @param frag
* @param v: a
* @param msg
*/
template <typename GRAPH_T, typename MESSAGE_T>
inline void SendMsgThroughIEdges(const GRAPH_T& frag,
const typename GRAPH_T::vertex_t& v,
const MESSAGE_T& msg) {
auto dsts = frag.IEDests(v);
const fid_t* ptr = dsts.begin;
typename GRAPH_T::Ver_T gid = frag.GetInnerVertexGid(v);
while (ptr != dsts.end) {
fid_t fid = *(ptr++);
to_send_[fid] << gid << msg;
}
}

/**
* @brief Communication via a crossing edge a->b. It sends message
* from a to b.
*
* @tparam GRAPH_T
* @tparam MESSAGE_T
* @param frag
* @param v: a
* @param msg
*/
template <typename GRAPH_T, typename MESSAGE_T>
inline void SendToNeighbors(const GRAPH_T& frag,
const typename GRAPH_T::vertex_t& v,
const MESSAGE_T& msg) {
auto dsts = frag.OEDests(v);
const fid_t* ptr = dsts.begin;
typename GRAPH_T::Ver_T gid = frag.GetInnerVertexGid(v);
while (ptr != dsts.end) {
fid_t fid = *(ptr++);
to_send_[fid] << gid << msg;
}
}

/**
* @brief Communication via crossing edges a->b and a<-c. It sends message
* from a to b and c.
*
* @tparam GRAPH_T
* @tparam MESSAGE_T
* @param frag
* @param v: a
* @param msg
*/
template <typename GRAPH_T, typename MESSAGE_T>
inline void SendMsgThroughEdges(const GRAPH_T& frag,
const typename GRAPH_T::vertex_t& v,
const MESSAGE_T& msg) {
auto dsts = frag.IOEDests(v);
const fid_t* ptr = dsts.begin;
typename GRAPH_T::Ver_T gid = frag.GetInnerVertexGid(v);
while (ptr != dsts.end) {
fid_t fid = *(ptr++);
to_send_[fid] << gid << msg;
}
}

/**
* @brief Get a message from message buffer.
*
* @tparam MESSAGE_T
* @param msg
*
* @return Return true if got a message, and false if no message left.
*/
template <typename MESSAGE_T>
inline bool GetMessage(MESSAGE_T& msg) {
while (cur_ != fnum_ && to_recv_[cur_].Empty()) {
++cur_;
}
if (cur_ == fnum_) {
return false;
}
to_recv_[cur_] >> msg;
return true;
}

/**
* @brief Get a message and its target vertex from message buffer.
*
* @tparam GRAPH_T
* @tparam MESSAGE_T
* @param frag
* @param v
* @param msg
*
* @return Return true if got a message, and false if no message left.
*/
template <typename GRAPH_T, typename MESSAGE_T>
inline bool GetMessage(const GRAPH_T& frag, typename GRAPH_T::vertex_t& v,
MESSAGE_T& msg) {
while (cur_ != fnum_ && to_recv_[cur_].Empty()) {
++cur_;
}
if (cur_ == fnum_) {
return false;
}
typename GRAPH_T::Ver_T gid;
to_recv_[cur_] >> gid >> msg;
frag.Gid2Vertex(gid, v);
return true;
}

protected:
fid_t fid() const { return fid_; }
fid_t fnum() const { return fnum_; }

std::vector<InArchive> to_send_;

private:
bool syncLengths() {
for (fid_t i = 0; i < fnum_; ++i) {
sent_size_ += to_send_[i].GetSize();
lengths_out_[i] = to_send_[i].GetSize();
}
if (force_continue_) {
++lengths_out_[fid_];
}
int terminate_flag = force_terminate_ ? 1 : 0;
int terminate_flag_sum;
MPI_Allreduce(&terminate_flag, &terminate_flag_sum, 1, MPI_INT, MPI_SUM,
comm_);
if (terminate_flag_sum > 0) {
terminate_info_.success = false;
sync_comm::AllGather(terminate_info_.info, comm_);
return true;
} else {
MPI_Allgather(&lengths_out_[0], fnum_ * sizeof(size_t), MPI_CHAR,
&lengths_in_[0], fnum_ * sizeof(size_t), MPI_CHAR, comm_);
for (auto s : lengths_in_) {
if (s != 0) {
return false;
}
}
return true;
}
}

std::vector<OutArchive> to_recv_;
fid_t cur_;

std::vector<size_t> lengths_out_;
std::vector<size_t> lengths_in_;

std::vector<MPI_Request> reqs_;
MPI_Comm comm_;

fid_t fid_;
fid_t fnum_;
CommSpec comm_spec_;
size_t sent_size_;
bool to_terminate_;
bool force_continue_;
bool force_terminate_;

TerminateInfo terminate_info_;
};

} // namespace platform

#endif // platform_PARALLEL_DEFAULT_MESSAGE_MANAGER_H_

#ifndef platform_PARALLEL_PARALLEL_MESSAGE_MANAGER_H_
#define platform_PARALLEL_PARALLEL_MESSAGE_MANAGER_H_

#include <mpi.h>

#include <array>
#include <atomic>
#include <memory>
#include <string>
#include <thread>
#include <utility>
#include <vector>

#include "platform/communication/sync_comm.h"
#include "platform/parallel/message_in_buffer.h"
#include "platform/parallel/message_manager_base.h"
#include "platform/parallel/thread_local_message_buffer.h"
#include "platform/serialization/in_archive.h"
#include "platform/serialization/out_archive.h"
#include "platform/utils/concurrent_queue.h"
#include "platform/worker/comm_spec.h"

namespace platform {

/**
* @brief A kind of parallel message manager.
*
* ParallelMessageManager support multi-threads to send messages concurrently
* with channels. Each channel contains a thread local message buffer.
*
* For each thread local message buffer, when accumulated a given amount of
* messages, the buffer will be sent through MPI.
*
* After a round of evaluation, there is a global barrier to determine whether
* the fixed point is reached.
*
*/

class ParallelMessageManager : public MessageManagerBase {
static constexpr size_t default_msg_send_block_size = 2 * 1023 * 1024;
static constexpr size_t default_msg_send_block_capacity = 2 * 1023 * 1024;

public:
ParallelMessageManager() : comm_(NULL_COMM) {}
~ParallelMessageManager() override {
if (ValidComm(comm_)) {
MPI_Comm_free(&comm_);
}
}

/**
* @brief Inherit
*/
void Init(MPI_Comm comm) override {
MPI_Comm_dup(comm, &comm_);
comm_spec_.Init(comm_);
fid_ = comm_spec_.fid();
fnum_ = comm_spec_.fnum();

force_terminate_ = false;
terminate_info_.Init(fnum_);

recv_queues_[0].SetProducerNum(fnum_);
recv_queues_[1].SetProducerNum(fnum_);

round_ = 0;

sent_size_ = 0;
}

/**
* @brief Inherit
*/
void Start() override { startRecvThread(); }

/**
* @brief Inherit
*/
void StartARound() override {
if (round_ != 0) {
waitSend();
auto& rq = recv_queues_[round_ % 2];
if (!to_self_.empty()) {
for (auto& iarc : to_self_) {
OutArchive oarc(std::move(iarc));
rq.Put(std::move(oarc));
}
to_self_.clear();
}
rq.DecProducerNum();
}
sent_size_ = 0;
startSendThread();
}

/**
* @brief Inherit
*/
void FinishARound() override {
sent_size_ = finishMsgFilling();
resetRecvQueue();
round_++;
}

/**
* @brief Inherit
*/
bool ToTerminate() override {
int flag[2];
flag[0] = 1;
if (sent_size_ == 0 && !force_continue_) {
flag[0] = 0;
}
flag[1] = force_terminate_ ? 1 : 0;
int ret[2];
MPI_Allreduce(&flag[0], &ret[0], 2, MPI_INT, MPI_SUM, comm_);
if (ret[1] > 0) {
terminate_info_.success = false;
sync_comm::AllGather(terminate_info_.info, comm_);
return true;
}
return (ret[0] == 0);
}

/**
* @brief Inherit
*/
void Finalize() override {
waitSend();
MPI_Barrier(comm_);
stopRecvThread();

MPI_Comm_free(&comm_);
comm_ = NULL_COMM;
}

/**
* @brief Inherit
*/
void ContinueProcessing() override { force_continue_ = true; }

/**
* @brief Inherit
*/
void ForceTerminate(const std::string& terminate_info) override {
force_terminate_ = true;
terminate_info_.info[comm_spec_.fid()] = terminate_info;
}

/**
* @brief Inherit
*/
const TerminateInfo& GetTerminateInfo() const override {
return terminate_info_;
}

/**
* @brief Inherit
*/
size_t GetMsgSize() const override { return sent_size_; }

/**
* @brief Init a set of channels, each channel is a thread local message
* buffer.
*
* @param channel_num Number of channels.
* @param block_size Size of each channel.
* @param block_cap Capacity of each channel.
*/
void InitChannels(int channel_num = 1,
size_t block_size = default_msg_send_block_size,
size_t block_cap = default_msg_send_block_capacity) {
channels_.resize(channel_num);
for (auto& channel : channels_) {
channel.Init(fnum_, this, block_size, block_cap);
}
}

std::vector<ThreadLocalMessageBuffer<ParallelMessageManager>>& Channels() {
return channels_;
}

/**
* @brief Send a buffer to a fragment.
*
* @param fid Destination fragment id.
* @param arc Message buffer.
*/
inline void SendRawMsgByFid(fid_t fid, InArchive&& arc) {
std::pair<fid_t, InArchive> item;
item.first = fid;
item.second = std::move(arc);
sending_queue_.Put(std::move(item));
}

/**
* @brief Send message to a fragment.
*
* @tparam MESSAGE_T Message type.
* @param dst_fid Destination fragment id.
* @param msg
* @param channelId
*/
template <typename MESSAGE_T>
inline void SendToFragment(fid_t dst_fid, const MESSAGE_T& msg,
int channel_id = 0) {
channels_[channel_id].SendToFragment<MESSAGE_T>(dst_fid, msg);
}

/**
* @brief SyncStateOnOuterVertex on a channel.
*
* @tparam GRAPH_T Graph type.
* @tparam MESSAGE_T Message type.
* @param frag Source fragment.
* @param v Source vertex.
* @param msg
* @param channel_id
*/
template <typename GRAPH_T, typename MESSAGE_T>
inline void SyncStateOnOuterVertex(const GRAPH_T& frag,
const typename GRAPH_T::vertex_t& v,
const MESSAGE_T& msg, int channel_id = 0) {
channels_[channel_id].SyncStateOnOuterVertex<GRAPH_T, MESSAGE_T>(frag, v,
msg);
}

template <typename GRAPH_T>
inline void SyncStateOnOuterVertex(const GRAPH_T& frag,
const typename GRAPH_T::vertex_t& v,
int channel_id = 0) {
channels_[channel_id].SyncStateOnOuterVertex<GRAPH_T>(frag, v);
}

/**
* @brief SendMsgThroughIEdges on a channel.
*
* @tparam GRAPH_T Graph type.
* @tparam MESSAGE_T Message type.
* @param frag Source fragment.
* @param v Source vertex.
* @param msg
* @param channel_id
*/
template <typename GRAPH_T, typename MESSAGE_T>
inline void SendMsgThroughIEdges(const GRAPH_T& frag,
const typename GRAPH_T::vertex_t& v,
const MESSAGE_T& msg, int channel_id = 0) {
channels_[channel_id].SendMsgThroughIEdges<GRAPH_T, MESSAGE_T>(frag, v,
msg);
}

/**
* @brief SendToNeighbors on a channel.
*
* @tparam GRAPH_T Graph type.
* @tparam MESSAGE_T Message type.
* @param frag Source fragment.
* @param v Source vertex.
* @param msg
* @param channel_id
*/
template <typename GRAPH_T, typename MESSAGE_T>
inline void SendToNeighbors(const GRAPH_T& frag,
const typename GRAPH_T::vertex_t& v,
const MESSAGE_T& msg, int channel_id = 0) {
channels_[channel_id].SendToNeighbors<GRAPH_T, MESSAGE_T>(frag, v,
msg);
}

/**
* @brief SendMsgThroughEdges on a channel.
*
* @tparam GRAPH_T Graph type.
* @tparam MESSAGE_T Message type.
* @param frag Source fragment.
* @param v Source vertex.
* @param msg
* @param channel_id
*/
template <typename GRAPH_T, typename MESSAGE_T>
inline void SendMsgThroughEdges(const GRAPH_T& frag,
const typename GRAPH_T::vertex_t& v,
const MESSAGE_T& msg, int channel_id = 0) {
channels_[channel_id].SendMsgThroughEdges<GRAPH_T, MESSAGE_T>(frag, v, msg);
}

/**
* @brief Get a bunch of messages, stored in a MessageInBuffer.
*
* @param buf Message buffer which holds a platform::OutArchive.
*/
inline bool GetMessageInBuffer(MessageInBuffer& buf) {
platform::OutArchive arc;
auto& que = recv_queues_[round_ % 2];
if (que.Get(arc)) {
buf.Init(std::move(arc));
return true;
} else {
return false;
}
}

/**
* @brief Parallel process all incoming messages with given function of last
* round.
*
* @tparam GRAPH_T Graph type.
* @tparam MESSAGE_T Message type.
* @tparam FUNC_T Function type.
* @param thread_num Number of threads.
* @param frag
* @param func
*/
template <typename GRAPH_T, typename MESSAGE_T, typename FUNC_T>
inline void ProcessMessages(int thread_num, const GRAPH_T& frag,
const FUNC_T& func) {
std::vector<std::thread> threads(thread_num);

for (int i = 0; i < thread_num; ++i) {
threads[i] = std::thread(
[&](int tid) {
typename GRAPH_T::Ver_T id;
typename GRAPH_T::vertex_t vertex(0);
MESSAGE_T msg;
auto& que = recv_queues_[round_ % 2];
OutArchive arc;
while (que.Get(arc)) {
while (!arc.Empty()) {
arc >> id >> msg;
frag.Gid2Vertex(id, vertex);
func(tid, vertex, msg);
}
}
},
i);
}

for (auto& thrd : threads) {
thrd.join();
}
}

/**
* @brief Parallel process all incoming messages with given function of last
* round.
*
* @tparam GRAPH_T Graph type.
* @tparam MESSAGE_T Message type.
* @tparam FUNC_T Function type.
* @param thread_num Number of threads.
* @param frag
* @param func
*/
template <typename MESSAGE_T, typename FUNC_T>
inline void ProcessMessages(int thread_num, const FUNC_T& func) {
std::vector<std::thread> threads(thread_num);

for (int i = 0; i < thread_num; ++i) {
threads[i] = std::thread(
[&](int tid) {
MESSAGE_T msg;
auto& que = recv_queues_[round_ % 2];
OutArchive arc;
while (que.Get(arc)) {
while (!arc.Empty()) {
arc >> msg;
func(tid, msg);
}
}
},
i);
}

for (auto& thrd : threads) {
thrd.join();
}
}

private:
void startSendThread() {
force_continue_ = false;
int round = round_;

CHECK_EQ(sending_queue_.Size(), 0);
sending_queue_.SetProducerNum(1);
send_thread_ = std::thread(
[this](int msg_round) {
std::vector<MPI_Request> reqs;
std::pair<fid_t, InArchive> item;
while (sending_queue_.Get(item)) {
if (item.second.GetSize() == 0) {
continue;
}
if (item.first == fid_) {
to_self_.emplace_back(std::move(item.second));
} else {
MPI_Request req;
sync_comm::isend_small_buffer<char>(
item.second.GetBuffer(), item.second.GetSize(),
comm_spec_.FragToWorker(item.first), msg_round, comm_, req);
reqs.push_back(req);
to_others_.emplace_back(std::move(item.second));
}
}
for (fid_t i = 0; i < fnum_; ++i) {
if (i == fid_) {
continue;
}
MPI_Request req;
sync_comm::isend_small_buffer<char>(
NULL, 0, comm_spec_.FragToWorker(i), msg_round, comm_, req);
reqs.push_back(req);
}
MPI_Waitall(reqs.size(), &reqs[0], MPI_STATUSES_IGNORE);
to_others_.clear();
},
round + 1);
}

void probeAllIncomingMessages() {
MPI_Status status;
while (true) {
MPI_Probe(MPI_ANY_SOURCE, MPI_ANY_TAG, comm_, &status);
if (status.MPI_SOURCE == comm_spec_.worker_id()) {
sync_comm::recv_small_buffer<char>(NULL, 0, status.MPI_SOURCE, 0,
comm_);
return;
}
int tag = status.MPI_TAG;
int count;
MPI_Get_count(&status, MPI_CHAR, &count);
if (count == 0) {
sync_comm::recv_small_buffer<char>(NULL, 0, status.MPI_SOURCE, tag,
comm_);
recv_queues_[tag % 2].DecProducerNum();
} else {
OutArchive arc(count);
sync_comm::recv_small_buffer<char>(arc.GetBuffer(), count,
status.MPI_SOURCE, tag, comm_);
recv_queues_[tag % 2].Put(std::move(arc));
}
}
}

int probeIncomingMessages() {
int gotMessage = 0;
int flag;
MPI_Status status;
while (true) {
MPI_Iprobe(MPI_ANY_SOURCE, MPI_ANY_TAG, comm_, &flag, &status);
if (flag) {
if (status.MPI_SOURCE == comm_spec_.worker_id()) {
sync_comm::recv_small_buffer<char>(NULL, 0, status.MPI_SOURCE, 0,
comm_);
return -1;
}
gotMessage = 1;
int tag = status.MPI_TAG;
int count;
MPI_Get_count(&status, MPI_CHAR, &count);
if (count == 0) {
sync_comm::recv_small_buffer<char>(NULL, 0, status.MPI_SOURCE, tag,
comm_);
recv_queues_[tag % 2].DecProducerNum();
} else {
OutArchive arc(count);
sync_comm::recv_small_buffer<char>(arc.GetBuffer(), count,
status.MPI_SOURCE, tag, comm_);
recv_queues_[tag % 2].Put(std::move(arc));
}
} else {
break;
}
}
return gotMessage;
}

void startRecvThread() {
recv_thread_ = std::thread([this]() {
#if 0
int idle_time = 0;
while (true) {
int gotMessage = probeIncomingMessages();
if (gotMessage == -1) {
break;
}
idle_time += static_cast<int>(gotMessage == 0);
if (idle_time > 10) {
poll(NULL, 0, 1);
idle_time = 0;
} else if (gotMessage == 0) {
#if __APPLE__
sched_yield();
#else
pthread_yield();
#endif
}
}
#else
probeAllIncomingMessages();
#endif
});
}

void stopRecvThread() {
sync_comm::send_small_buffer<char>(NULL, 0, comm_spec_.worker_id(), 0,
comm_);
recv_thread_.join();
}

inline size_t finishMsgFilling() {
size_t ret = 0;
for (auto& channel : channels_) {
channel.FlushMessages();
ret += channel.SentMsgSize();
channel.Reset();
}
sending_queue_.DecProducerNum();
return ret;
}

void resetRecvQueue() {
auto& curr_recv_queue = recv_queues_[round_ % 2];
if (round_) {
OutArchive arc;
while (curr_recv_queue.Get(arc)) {}
}
curr_recv_queue.SetProducerNum(fnum_);
}

void waitSend() { send_thread_.join(); }

fid_t fid_;
fid_t fnum_;
CommSpec comm_spec_;

MPI_Comm comm_;

std::vector<InArchive> to_self_;
std::vector<InArchive> to_others_;
std::vector<ThreadLocalMessageBuffer<ParallelMessageManager>> channels_;
int round_;

BlockingQueue<std::pair<fid_t, InArchive>> sending_queue_;
std::thread send_thread_;

std::array<BlockingQueue<OutArchive>, 2> recv_queues_;
std::thread recv_thread_;

bool force_continue_;
size_t sent_size_;

bool force_terminate_;
TerminateInfo terminate_info_;
};

} // namespace platform

#endif // platform_PARALLEL_PARALLEL_MESSAGE_MANAGER_H_

#ifndef platform_UTILS_VERTEX_ARRAY_H_
#define platform_UTILS_VERTEX_ARRAY_H_

#include <algorithm>
#include <utility>

#include "platform/config.h"
#include "platform/serialization/in_archive.h"
#include "platform/serialization/out_archive.h"
#include "platform/utils/gcontainer.h"

namespace platform {

/**
* @brief A Vertex object only contains id of a vertex.
* It will be used when iterating vertices of a fragment and
* accessing data and neighbor of a vertex.
*
* @tparam T Vertex ID type.
*/
template <typename T>
class Vertex {
public:
Vertex() = default;
DEV_HOST explicit Vertex(const T& value) noexcept : value_(value) {}

~Vertex() = default;

DEV_HOST_INLINE Vertex& operator=(const T& value) noexcept {
value_ = value;
return *this;
}

DEV_HOST_INLINE Vertex& operator++() noexcept {
value_++;
return *this;
}

DEV_HOST_INLINE Vertex operator++(int) {
Vertex res(value_);
value_++;
return res;
}

DEV_HOST_INLINE Vertex& operator--() noexcept {
value_--;
return *this;
}
DEV_HOST_INLINE Vertex operator--(int) noexcept {
Vertex res(value_);
value_--;
return res;
}

DEV_HOST_INLINE Vertex operator+(size_t offset) const noexcept {
Vertex res(value_ + offset);
return res;
}

DEV_HOST_INLINE bool operator==(const Vertex& rhs) const {
return value_ == rhs.value_;
}

DEV_HOST_INLINE bool operator!=(const Vertex& rhs) const {
return value_ != rhs.value_;
}

DEV_HOST_INLINE void Swap(Vertex& rhs) {
#ifdef __CUDACC__
thrust::swap(value_, rhs.value_);
#else
std::swap(value_, rhs.value_);
#endif
}

DEV_HOST_INLINE bool operator<(const Vertex& rhs) const {
return value_ < rhs.value_;
}

DEV_HOST_INLINE T GetValue() const { return value_; }

DEV_HOST_INLINE void SetValue(T value) { value_ = value; }

friend InArchive& operator<<(InArchive& archive, const Vertex& h) {
archive << h.value_;
return archive;
}

friend OutArchive& operator>>(OutArchive& archive, Vertex& h) {
archive >> h.value_;
return archive;
}

private:
T value_{};
};

template <typename T>
bool operator<(Vertex<T> const& lhs, Vertex<T> const& rhs) {
return lhs.GetValue() < rhs.GetValue();
}

template <typename T>
bool operator==(Vertex<T> const& lhs, Vertex<T> const& rhs) {
return lhs.GetValue() == rhs.GetValue();
}

template <typename T>
class VertexRange {
public:
using vertex_t = Vertex<T>;

DEV_HOST VertexRange() {}
DEV_HOST VertexRange(const T& begin, const T& end)
: begin_(begin), end_(end) {}
DEV_HOST VertexRange(const VertexRange& r) : begin_(r.begin_), end_(r.end_) {}

class iterator {
using reference_type = Vertex<T>&;

private:
Vertex<T> cur_;

public:
DEV_HOST iterator() noexcept : cur_() {}
DEV_HOST explicit iterator(const T& v) noexcept : cur_(v) {}

DEV_HOST_INLINE reference_type operator*() noexcept { return cur_; }

DEV_HOST_INLINE iterator& operator++() noexcept {
++cur_;
return *this;
}

DEV_HOST_INLINE iterator operator++(int) noexcept {
iterator ret = *this;
++*this;
return ret;
}

DEV_HOST_INLINE iterator& operator--() noexcept {
--cur_;
return *this;
}

DEV_HOST_INLINE iterator operator--(int) noexcept {
iterator ret = *this;
--*this;
return ret;
}

DEV_HOST_INLINE iterator operator+(size_t offset) const noexcept {
return iterator(cur_.GetValue() + offset);
}

DEV_HOST bool operator==(const iterator& rhs) const noexcept {
return cur_ == rhs.cur_;
}

DEV_HOST bool operator!=(const iterator& rhs) const noexcept {
return cur_ != rhs.cur_;
}

DEV_HOST bool operator<(const iterator& rhs) const noexcept {
return cur_.GetValue() < rhs.cur_.GetValue();
}
};

DEV_HOST_INLINE iterator begin() const { return iterator(begin_); }

DEV_HOST_INLINE iterator end() const { return iterator(end_); }

DEV_HOST_INLINE size_t size() const { return end_ - begin_; }

DEV_HOST void Swap(VertexRange& rhs) {
#ifdef __CUDACC__
thrust::swap(begin_, rhs.begin_);
thrust::swap(end_, rhs.end_);
#else
std::swap(begin_, rhs.begin_);
std::swap(end_, rhs.end_);
#endif
}

DEV_HOST void SetRange(const T& begin, const T& end) {
begin_ = begin;
end_ = end;
}

DEV_HOST const T& begin_value() const { return begin_; }

DEV_HOST const T& end_value() const { return end_; }

inline bool Contain(const Vertex<T>& v) const {
return begin_ <= v.GetValue() && v.GetValue() < end_;
}

inline friend InArchive& operator<<(InArchive& in_archive,
const VertexRange<T>& range) {
in_archive << range.begin_ << range.end_;
return in_archive;
}

inline friend OutArchive& operator>>(OutArchive& out_archive,
VertexRange<T>& range) {
out_archive >> range.begin_ >> range.end_;
return out_archive;
}

private:
T begin_, end_;
};

template <typename Ver_T>
class DualVertexRange {
public:
using vertex_t = Vertex<Ver_T>;

DualVertexRange() {}

DualVertexRange(const Ver_T& head_begin, const Ver_T& head_end,
const Ver_T& tail_begin, const Ver_T& tail_end) {
SetRange(head_begin, head_end, tail_begin, tail_end);
}

void SetRange(const Ver_T& head_begin, const Ver_T& head_end,
const Ver_T& tail_begin, const Ver_T& tail_end) {
head_begin_ = head_begin;
tail_begin_ = tail_begin;
head_end_ = std::max(head_begin_, head_end);
tail_end_ = std::max(tail_begin_, tail_end);

if (head_begin_ > tail_begin_) {
std::swap(head_begin_, tail_begin_);
std::swap(head_end_, tail_end_);
}
if (head_end_ >= tail_begin_) {
head_end_ = tail_end_;
tail_begin_ = tail_end_;
}
}

class iterator {
using reference_type = const Vertex<Ver_T>&;

private:
Vertex<Ver_T> cur_;
Ver_T head_end_;
Ver_T tail_begin_;

public:
iterator() noexcept : cur_() {}
explicit iterator(const Ver_T& v) noexcept : cur_(v) {}
explicit iterator(const Ver_T& v, const Ver_T& x, const Ver_T& y) noexcept
: cur_(v), head_end_(x), tail_begin_(y) {}

inline reference_type operator*() noexcept { return cur_; }

inline iterator& operator++() noexcept {
++cur_;
if (cur_.GetValue() == head_end_) {
cur_.SetValue(tail_begin_);
}
return *this;
}

inline iterator operator++(int) noexcept {
Ver_T new_value = cur_.GetValue() + 1;
if (new_value == head_end_) {
new_value = tail_begin_;
}
return iterator(new_value, head_end_, tail_begin_);
}

inline iterator& operator--() noexcept {
if (cur_.GetValue() == tail_begin_) {
cur_.SetValue(head_end_);
}
--cur_;
return *this;
}

inline iterator operator--(int) noexcept {
return iterator(cur_.GetValue()--, head_end_, tail_begin_);
}

iterator operator+(size_t offset) noexcept {
Ver_T new_value = cur_.GetValue() + offset;
if (cur_.GetValue() < head_end_ && new_value >= head_end_) {
new_value = offset - (head_end_ - cur_.GetValue()) + tail_begin_;
}
return iterator(new_value, head_end_, tail_begin_);
}
bool operator==(const iterator& rhs) noexcept { return cur_ == rhs.cur_; }

bool operator!=(const iterator& rhs) noexcept { return cur_ != rhs.cur_; }
};

iterator begin() const {
return iterator(head_begin_, head_end_, tail_begin_);
}

iterator end() const { return iterator(tail_end_); }

VertexRange<Ver_T> head() const {
return VertexRange<Ver_T>(head_begin_, head_end_);
}
VertexRange<Ver_T> tail() const {
return VertexRange<Ver_T>(tail_begin_, tail_end_);
}

const Ver_T begin_value() const { return head_begin_; }

const Ver_T end_value() const { return tail_end_; }

bool Contain(const Vertex<Ver_T>& v) const {
return (head_begin_ <= v.GetValue() && v.GetValue() < head_end_) ||
(tail_begin_ <= v.GetValue() && v.GetValue() < tail_end_);
}

Ver_T size() const {
return (head_end_ - head_begin_) + (tail_end_ - tail_begin_);
}

friend InArchive& operator<<(InArchive& in_archive,
const DualVertexRange<Ver_T>& range) {
in_archive << range.head_begin_ << range.head_end_ << range.tail_begin_
<< range.tail_end_;
return in_archive;
}

friend OutArchive& operator>>(OutArchive& out_archive,
DualVertexRange<Ver_T>& range) {
out_archive >> range.head_begin_ >> range.head_end_ >> range.tail_begin_ >>
range.tail_end_;
return out_archive;
}

private:
Ver_T head_begin_;
Ver_T head_end_;
Ver_T tail_begin_;
Ver_T tail_end_;
};

template <typename Ver_T>
inline InArchive& operator<<(InArchive& in_archive,
const DualVertexRange<Ver_T>& range) {
in_archive.AddBytes(&range, sizeof(DualVertexRange<Ver_T>));
return in_archive;
}

/**
* @brief A discontinuous vertices collection representation. An increasing
* labeled(but no need to be continuous) vertices must be provided to construct
* the VertexVector.
*
* @tparam T Vertex ID type.
*/
template <typename T>
using VertexVector = std::vector<Vertex<T>>;

template <typename VERTEX_SET_T, typename T>
class VertexArray {};

template <typename Ver_T, typename T>
class VertexArray<VertexRange<Ver_T>, T> : public Array<T, Allocator<T>> {
using Base = Array<T, Allocator<T>>;

public:
VertexArray() : Base(), fake_start_(NULL) {}
explicit VertexArray(const VertexRange<Ver_T>& range)
: Base(range.size()), range_(range) {
fake_start_ = Base::data() - range_.begin_value();
}
VertexArray(const VertexRange<Ver_T>& range, const T& value)
: Base(range.size(), value), range_(range) {
fake_start_ = Base::data() - range_.begin_value();
}

~VertexArray() = default;

void Init(const VertexRange<Ver_T>& range) {
Base::clear();
Base::resize(range.size());
range_ = range;
fake_start_ = Base::data() - range_.begin_value();
}

void Init(const VertexRange<Ver_T>& range, const T& value) {
Base::clear();
Base::resize(range.size(), value);
range_ = range;
fake_start_ = Base::data() - range_.begin_value();
}

void SetValue(VertexRange<Ver_T>& range, const T& value) {
std::fill_n(&Base::data()[range.begin_value() - range_.begin_value()],
range.size(), value);
}
void SetValue(const Vertex<Ver_T>& loc, const T& value) {
fake_start_[loc.GetValue()] = value;
}

void SetValue(const T& value) {
std::fill_n(Base::data(), Base::size(), value);
}
inline T& operator[](const Vertex<Ver_T>& loc) {
return fake_start_[loc.GetValue()];
}
inline const T& operator[](const Vertex<Ver_T>& loc) const {
return fake_start_[loc.GetValue()];
}

void Swap(VertexArray& rhs) {
Base::swap((Base&) rhs);
range_.Swap(rhs.range_);
std::swap(fake_start_, rhs.fake_start_);
}

void Clear() {
VertexArray ga;
this->Swap(ga);
}

const VertexRange<Ver_T>& GetVertexRange() const { return range_; }

private:
void Resize() {}

VertexRange<Ver_T> range_;
T* fake_start_;
};

template <typename Ver_T, typename T>
class VertexArray<DualVertexRange<Ver_T>, T> {
public:
VertexArray() : head_(), tail_() {}
explicit VertexArray(const DualVertexRange<Ver_T>& range)
: head_(range.head()), tail_(range.tail()) {
initMid();
}
VertexArray(const DualVertexRange<Ver_T>& range, const T& value)
: head_(range.head(), value), tail_(range.tail(), value) {
initMid();
}
~VertexArray() = default;

void Init(const VertexRange<Ver_T>& range) {
head_.Init(range);
tail_.Init(VertexRange<Ver_T>(mid_, mid_));
initMid();
}

void Init(const DualVertexRange<Ver_T>& range) {
head_.Init(range.head());
tail_.Init(range.tail());
initMid();
}

void Init(const VertexRange<Ver_T>& range, const T& value) {
head_.Init(range, value);
tail_.Init(VertexRange<Ver_T>(mid_, mid_));
initMid();
}

void Init(const DualVertexRange<Ver_T>& range, const T& value) {
head_.Init(range.head(), value);
tail_.Init(range.tail(), value);
initMid();
}

inline T& operator[](const Vertex<Ver_T>& loc) {
return loc.GetValue() < mid_ ? head_[loc] : tail_[loc];
}
inline const T& operator[](const Vertex<Ver_T>& loc) const {
return loc.GetValue() < mid_ ? head_[loc] : tail_[loc];
}

void Swap(VertexArray& rhs) {
head_.Swap(rhs.head_);
tail_.Swap(rhs.tail_);
std::swap(mid_, rhs.mid_);
}

void Clear() {
head_.Clear();
tail_.Clear();
}

void SetValue(const T& value) {
head_.SetValue(value);
tail_.SetValue(value);
}

private:
void initMid() { mid_ = head_.GetVertexRange().end_value(); }

VertexArray<VertexRange<Ver_T>, T> head_;
VertexArray<VertexRange<Ver_T>, T> tail_;
Ver_T mid_;
};

} // namespace platform

namespace std {
template <typename T>
struct hash<platform::Vertex<T>> {
inline size_t operator()(const platform::Vertex<T>& obj) const {
return hash<T>()(obj.GetValue());
}
};

} // namespace std

#endif // platform_UTILS_VERTEX_ARRAY_H_

#ifndef platform_UTILS_VERTEX_SET_H_
#define platform_UTILS_VERTEX_SET_H_
#include <utility>
#include "platform/utils/bitset.h"
#include "platform/utils/thread_pool.h"
#include "platform/utils/vertex_array.h"
namespace platform {
/**
* @brief A vertex set with dense vertices.
*
* @tparam VERTEX_SET_T Vertex set type.
*/
template <typename VERTEX_SET_T>
class NodeSet {};
template <typename Ver_T>
class NodeSet<VertexRange<Ver_T>> {
public:
NodeSet() = default;
explicit NodeSet(const VertexRange<Ver_T>& range)
: beg_(range.begin_value()), end_(range.end_value()), bs_(end_ - beg_) {}
~NodeSet() = default;
void Init(const VertexRange<Ver_T>& range, ThreadPool& thread_pool) {
beg_ = range.begin_value();
end_ = range.end_value();
bs_.init(end_ - beg_);
bs_.parallel_clear(thread_pool);
}
void Init(const VertexVector<Ver_T>& vertices, ThreadPool& thread_pool) {
if (vertices.size() == 0)
return;
beg_ = vertices[0].GetValue();
end_ = vertices[vertices.size() - 1].GetValue();
bs_.init(end_ - beg_ + 1);
bs_.parallel_clear(thread_pool);
}
void Init(const VertexRange<Ver_T>& range) {
beg_ = range.begin_value();
end_ = range.end_value();
bs_.init(end_ - beg_);
bs_.clear();
}
void Init(const VertexVector<Ver_T>& vertices) {
if (vertices.size() == 0)
return;
beg_ = vertices[0].GetValue();
end_ = vertices[vertices.size() - 1].GetValue();
bs_.init(end_ - beg_ + 1);
bs_.clear();
}
void Insert(Vertex<Ver_T> u) { bs_.set_bit(u.GetValue() - beg_); }
bool InsertWithRet(Vertex<Ver_T> u) {
return bs_.set_bit_with_ret(u.GetValue() - beg_);
}
void Erase(Vertex<Ver_T> u) { bs_.reset_bit(u.GetValue() - beg_); }
bool EraseWithRet(Vertex<Ver_T> u) {
return bs_.reset_bit_with_ret(u.GetValue() - beg_);
}
bool Exist(Vertex<Ver_T> u) const { return bs_.get_bit(u.GetValue() - beg_); }
VertexRange<Ver_T> Range() const { return VertexRange<Ver_T>(beg_, end_); }
size_t Count() const { return bs_.count(); }
size_t ParallelCount(ThreadPool& thread_pool) const {
return bs_.parallel_count(thread_pool);
}
size_t PartialCount(Ver_T beg, Ver_T end) const {
return bs_.partial_count(beg - beg_, end - beg_);
}
size_t ParallelPartialCount(ThreadPool& thread_pool, Ver_T beg,
Ver_T end) const {
return bs_.parallel_partial_count(thread_pool, beg - beg_, end - beg_);
}
void Clear() { bs_.clear(); }
void ParallelClear(ThreadPool& thread_pool) {
bs_.parallel_clear(thread_pool);
}
void Swap(NodeSet& rhs) {
std::swap(beg_, rhs.beg_);
std::swap(end_, rhs.end_);
bs_.swap(rhs.bs_);
}
Bitset& GetBitset() { return bs_; }
const Bitset& GetBitset() const { return bs_; }
bool Empty() const { return bs_.empty(); }
bool PartialEmpty(Ver_T beg, Ver_T end) const {
return bs_.partial_empty(beg - beg_, end - beg_);
}
private:
Ver_T beg_;
Ver_T end_;
Bitset bs_;
};
template <typename Ver_T>
class NodeSet<DualVertexRange<Ver_T>> {
public:
NodeSet() = default;
explicit NodeSet(const DualVertexRange<Ver_T>& range)
: head_beg_(range.head().begin_value()),
head_end_(range.head().end_value()),
tail_beg_(range.tail().begin_value()),
tail_end_(range.tail().end_value()),
head_bs_(head_end_ - head_beg_),
tail_bs_(tail_end_ - tail_beg_) {}
~NodeSet() = default;
void Init(const VertexRange<Ver_T>& range, ThreadPool& thread_pool) {
head_beg_ = range.head().begin_value();
head_end_ = range.head().end_value();
tail_beg_ = range.tail().begin_value();
tail_end_ = range.tail().end_value();
head_bs_.init(head_end_ - head_beg_);
tail_bs_.init(tail_end_ - tail_beg_);
head_bs_.parallel_clear(thread_pool);
tail_bs_.parallel_clear(thread_pool);
}
void Init(const VertexVector<Ver_T>& vertices, ThreadPool& thread_pool) {
if (vertices.size() == 0)
return;
head_beg_ = vertices[0].GetValue();
head_end_ = vertices[vertices.size() - 1].GetValue();
head_bs_.init(head_end_ - head_beg_ + 1);
head_bs_.parallel_clear(thread_pool);
tail_beg_ = head_end_;
tail_end_ = tail_beg_;
}
void Init(const DualVertexRange<Ver_T>& range) {
head_beg_ = range.head().begin_value();
head_end_ = range.head().end_value();
tail_beg_ = range.tail().begin_value();
tail_end_ = range.tail().end_value();
head_bs_.init(head_end_ - head_beg_);
tail_bs_.init(tail_end_ - tail_beg_);
head_bs_.clear();
tail_bs_.clear();
}
void Init(const VertexVector<Ver_T>& vertices) {
if (vertices.size() == 0)
return;
head_beg_ = vertices[0].GetValue();
head_end_ = vertices[vertices.size() - 1].GetValue();
head_bs_.init(head_end_ - head_beg_ + 1);
head_bs_.clear();
tail_beg_ = head_end_;
tail_end_ = tail_beg_;
}
void Insert(Vertex<Ver_T> u) {
if (u.GetValue() < head_end_) {
head_bs_.set_bit(u.GetValue() - head_beg_);
} else {
tail_bs_.set_bit(u.GetValue() - tail_beg_);
}
}
bool InsertWithRet(Vertex<Ver_T> u) {
if (u.GetValue() < head_end_) {
return head_bs_.set_bit_with_ret(u.GetValue() - head_beg_);
} else {
return tail_bs_.set_bit_with_ret(u.GetValue() - tail_beg_);
}
}
void Erase(Vertex<Ver_T> u) {
if (u.GetValue() < head_end_) {
head_bs_.reset_bit(u.GetValue() - head_beg_);
} else {
tail_bs_.reset_bit(u.GetValue() - tail_beg_);
}
}
bool EraseWithRet(Vertex<Ver_T> u) {
if (u.GetValue() < head_end_) {
return head_bs_.reset_bit_with_ret(u.GetValue() - head_beg_);
} else {
return tail_bs_.reset_bit_with_ret(u.GetValue() - tail_beg_);
}
}
bool Exist(Vertex<Ver_T> u) const {
if (u.GetValue() < head_end_) {
return head_bs_.get_bit(u.GetValue() - head_beg_);
} else {
return tail_bs_.get_bit(u.GetValue() - tail_beg_);
}
}
DualVertexRange<Ver_T> Range() const {
return DualVertexRange<Ver_T>(head_beg_, head_end_, tail_beg_, tail_end_);
}
size_t Count() const { return head_bs_.count() + tail_bs_.count(); }
size_t ParallelCount(ThreadPool& thread_pool) const {
return head_bs_.parallel_count(thread_pool) +
tail_bs_.parallel_count(thread_pool);
}
size_t PartialCount(Ver_T beg, Ver_T end) const {
size_t ret = 0;
if (beg < head_end_) {
ret += head_bs_.partial_count(std::max(beg, head_beg_) - head_beg_,
std::min(end, head_end_) - head_beg_);
}
if (end > tail_beg_) {
ret += tail_bs_.partial_count(std::max(beg, tail_beg_) - tail_beg_,
std::min(end, tail_end_) - tail_beg_);
}
return ret;
}
size_t ParallelPartialCount(ThreadPool& thread_pool, Ver_T beg,
Ver_T end) const {
size_t ret = 0;
if (beg < head_end_) {
ret += head_bs_.parallel_partial_count(
thread_pool, std::max(beg, head_beg_) - head_beg_,
std::min(end, head_end_) - head_beg_);
}
if (end > tail_beg_) {
ret += tail_bs_.parallel_partial_count(
thread_pool, std::max(beg, tail_beg_) - tail_beg_,
std::min(end, tail_end_) - tail_beg_);
}
return ret;
}

#ifndef platform_APP_MUTATION_CONTEXT_H_
#define platform_APP_MUTATION_CONTEXT_H_

#include <platform/config.h>
#include <platform/fragment/basic_fragment_mutator.h>
#include "platform/app/context_base.h"

namespace platform {
template <typename TEST_T>
class MutationContext : public ContextBase {
using fragment_t = TEST_T;
using oid_t = typename TEST_T::oid_t;
using Ver_T = typename TEST_T::Ver_T;
using vdata_t = typename TEST_T::vdata_t;
using edata_t = typename TEST_T::edata_t;
using vertex_map_t = typename TEST_T::vertex_map_t;
using partitioner_t = typename vertex_map_t::partitioner_t;
using vertex_t = typename TEST_T::vertex_t;

using oid_list = typename ShuffleBuffer<oid_t>::type;
using vid_list = typename ShuffleBuffer<Ver_T>::type;
using vdata_list = typename ShuffleBuffer<vdata_t>::type;
using edata_list = typename ShuffleBuffer<edata_t>::type;

public:
explicit MutationContext(const fragment_t& fragment)
: fragment_(fragment),
vm_ptr_(fragment.GetVertexMap()),
partitioner_(vm_ptr_->GetPartitioner()) {
fid_t fnum = fragment_->fnum();
id_to_add_.resize(fnum);
vdata_to_add_.resize(fnum);

esrc_to_add_.resize(fnum);
edst_to_add_.resize(fnum);
edata_to_add_.resize(fnum);

id_to_update_.resize(fnum);
vdata_to_update_.resize(fnum);

esrc_to_update_.resize(fnum);
edst_to_update_.resize(fnum);
edata_to_update_.resize(fnum);

id_to_remove_.resize(fnum);

esrc_to_remove_.resize(fnum);
edst_to_remove_.resize(fnum);
}

void add_vertex(const oid_t& id, const vdata_t& data) {
fid_t fid = partitioner_.GetPartitionId(id);
id_to_add_[fid].push_back(id);
vdata_to_add_[fid].push_back(data);
}

void add_edge(const oid_t& src, const oid_t& dst, const edata_t& data) {
fid_t src_fid = partitioner_.GetPartitionId(src);
fid_t dst_fid = partitioner_.GetPartitionId(dst);
esrc_to_add_[src_fid].push_back(src);
edst_to_add_[src_fid].push_back(dst);
edata_to_add_[src_fid].push_back(data);
if (src_fid != dst_fid) {
esrc_to_add_[dst_fid].push_back(src);
edst_to_add_[dst_fid].push_back(dst);
edata_to_add_[dst_fid].push_back(data);
}
}

void add_edge(const vertex_t& src, const vertex_t& dst, const edata_t& data) {
oid_t src_oid = fragment_.GetId(src);
oid_t dst_oid = fragment_.GetId(dst);
add_edge(src_oid, dst_oid, data);
}

void update_vertex(const oid_t& id, const vdata_t& data) {
Ver_T gid;
if (vm_ptr_->GetGid(id, gid)) {
parsed_vertices_to_update_.emplace_back(gid, data);
} else {
fid_t fid = partitioner_.GetPartitionId(id);
id_to_update_[fid].push_back(id);
vdata_to_update_[fid].push_back(data);
}
}

void update_vertex(const vertex_t& v, const vdata_t& data) {
parsed_vertices_to_update_.emplace_back(fragment_.Vertex2Gid(v), data);
}

void update_edge(const oid_t& src, const oid_t& dst, const edata_t& data) {
fid_t src_fid = partitioner_.GetPartitionId(src);
fid_t dst_fid = partitioner_.GetPartitionId(dst);
esrc_to_update_[src_fid].push_back(src);
edst_to_update_[src_fid].push_back(dst);
edata_to_update_[src_fid].push_back(data);
if (src_fid != dst_fid) {
esrc_to_update_[dst_fid].push_back(src);
edst_to_update_[dst_fid].push_back(dst);
edata_to_update_[dst_fid].push_back(data);
}
}

void update_edge(const vertex_t& src, const vertex_t& dst,
const edata_t& data) {
oid_t src_oid = fragment_.GetId(src);
oid_t dst_oid = fragment_.GetId(dst);
update_edge(src_oid, dst_oid, data);
}

void remove_vertex(const oid_t& id) {
Ver_T gid;
if (vm_ptr_->GetGid(id, gid)) {
parsed_Ver_To_remove_.push_back(gid);
} else {
fid_t fid = partitioner_.GetPartitionId(id);
id_to_remove_[fid].push_back(id);
}
}

void remove_vertex(const vertex_t& v) {
parsed_Ver_To_remove_.push_back(fragment_.Vertex2Gid(v));
}

void remove_edge(const oid_t& src, const oid_t& dst) {
fid_t src_fid = partitioner_.GetPartitionId(src);
fid_t dst_fid = partitioner_.GetPartitionId(dst);
esrc_to_remove_[src_fid].push_back(src);
edst_to_remove_[src_fid].push_back(dst);
if (src_fid != dst_fid) {
esrc_to_remove_[dst_fid].push_back(src);
edst_to_remove_[dst_fid].push_back(dst);
}
}

void remove_edge(const vertex_t& src, const vertex_t& dst) {
oid_t src_oid = fragment_.GetId(src);
oid_t dst_oid = fragment_.GetId(dst);
remove_edge(src_oid, dst_oid);
}
void apply_mutation(std::shared_ptr<fragment_t> fragment,
const CommSpec& comm_spec) {
{
CommSpec dup_comm_spec(comm_spec);
dup_comm_spec.Dup();
int local_to_mutate = 1;
if (id_to_add_.empty() && esrc_to_add_.empty() &&
parsed_vertices_to_update_.empty() && id_to_update_.empty() &&
esrc_to_update_.empty() && parsed_Ver_To_remove_.empty() &&
id_to_remove_.empty() && esrc_to_remove_.empty()) {
local_to_mutate = 0;
}
int global_to_mutate;
MPI_Allreduce(&local_to_mutate, &global_to_mutate, 1, MPI_INT, MPI_SUM,
comm_spec.comm());
if (global_to_mutate == 0) {
return;
}
}
BasicFragmentMutator<fragment_t> mutator(comm_spec, fragment);
mutator.AddVerticesToRemove(std::move(parsed_Ver_To_remove_));
mutator.AddVerticesToUpdate(std::move(parsed_vertices_to_update_));
mutator.Start();
mutator.AddVertices(std::move(id_to_add_), std::move(vdata_to_add_));
mutator.RemoveVertices(std::move(id_to_remove_));
mutator.UpdateVertices(std::move(id_to_update_),
std::move(vdata_to_update_));
mutator.AddEdges(std::move(esrc_to_add_), std::move(edst_to_add_),
std::move(edata_to_update_));
mutator.RemoveEdges(std::move(esrc_to_remove_), std::move(edst_to_remove_));
mutator.UpdateEdges(std::move(esrc_to_update_), std::move(edst_to_update_),
std::move(edata_to_update_));
mutator.MutateFragment();
id_to_add_.clear();
vdata_to_add_.clear();
esrc_to_add_.clear();
edst_to_add_.clear();
edata_to_add_.clear();
parsed_vertices_to_update_.clear();
id_to_update_.clear();
vdata_to_update_.clear();
esrc_to_update_.clear();
edst_to_update_.clear();
edata_to_update_.clear();
parsed_Ver_To_remove_.clear();
id_to_remove_.clear();
esrc_to_remove_.clear();
edst_to_remove_.clear();
}

private:
const fragment_t& fragment_;
const std::shared_ptr<vertex_map_t> vm_ptr_;
const partitioner_t& partitioner_;

std::vector<oid_list> id_to_add_;
std::vector<vdata_list> vdata_to_add_;

std::vector<oid_list> esrc_to_add_;
std::vector<oid_list> edst_to_add_;
std::vector<edata_list> edata_to_add_;

std::vector<internal::Vertex<Ver_T, vdata_t>> parsed_vertices_to_update_;
std::vector<oid_list> id_to_update_;
std::vector<vdata_list> vdata_to_update_;

std::vector<oid_list> esrc_to_update_;
std::vector<oid_list> edst_to_update_;
std::vector<edata_list> edata_to_update_;

std::vector<Ver_T> parsed_Ver_To_remove_;
std::vector<oid_list> id_to_remove_;
std::vector<oid_list> esrc_to_remove_;
std::vector<oid_list> edst_to_remove_;
};

} // namespace platform

#endif // platform_APP_MUTATION_CONTEXT_H_